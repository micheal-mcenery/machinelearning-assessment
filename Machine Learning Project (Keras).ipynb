{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6759213",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:3rem;color:rgb(0, 91, 94);text-align:center;\">Machine Learning Project (Keras)</h1>\n",
    "<hr style=\\\"border-top: 1px solid rgb(0, 91, 94);\\\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be96801",
   "metadata": {},
   "source": [
    "This section of the project has four key requirements, each of which have been satisfied below:\n",
    "\n",
    "- On the keras website, there is an example of time-series anomaly detection. Re-create this example in a notebook of your own, explaining the concepts.\n",
    "\n",
    "- Clearly explain each keras function used, referring to the documentation.\n",
    "\n",
    "- Include an introduction to your notebook, setting the context and describing what the reader can expect as they read down through the notebook.\n",
    "\n",
    "- Include a conclusion section where you suggest improvements you could make to the analysis in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11332a9f",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The purpose of this notebook is to recreate the time-series anomaly detection example found on the Keras website, explain the main concepts, and define the purpose of each of the functions used. \"Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow. It was developed with a focus on enabling fast experimentation\" (https://keras.io/about/). For each section of the code below, both the concept of the section is exaplained, as well as any keras functions found within."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edf7dac",
   "metadata": {},
   "source": [
    "### Recreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b15792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e15190f",
   "metadata": {},
   "source": [
    "#### Concept: Setup\n",
    "\n",
    "The purpose of this section is to import the different tools that will be needed in this example.\n",
    "\n",
    "Numpy, Pandas, Keras (Layers), and Matplotlib are imported for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e0113e",
   "metadata": {},
   "source": [
    "#### Keras Functions:\n",
    "\n",
    "- from tensorflow import keras (imports Keras functionality)\n",
    "- from tensorflow.keras import layers (imports Keras Layers, basic building blocks of neural networks, https://keras.io/api/layers/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e6d8f1",
   "metadata": {},
   "source": [
    "<hr style=\\\"border-top: 1px solid rgb(0, 91, 94);\\\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f37c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_url_root = \"https://raw.githubusercontent.com/numenta/NAB/master/data/\"\n",
    "\n",
    "df_small_noise_url_suffix = \"artificialNoAnomaly/art_daily_small_noise.csv\"\n",
    "df_small_noise_url = master_url_root + df_small_noise_url_suffix\n",
    "df_small_noise = pd.read_csv(\n",
    "    df_small_noise_url, parse_dates=True, index_col=\"timestamp\"\n",
    ")\n",
    "\n",
    "df_daily_jumpsup_url_suffix = \"artificialWithAnomaly/art_daily_jumpsup.csv\"\n",
    "df_daily_jumpsup_url = master_url_root + df_daily_jumpsup_url_suffix\n",
    "df_daily_jumpsup = pd.read_csv(\n",
    "    df_daily_jumpsup_url, parse_dates=True, index_col=\"timestamp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151b77c7",
   "metadata": {},
   "source": [
    "#### Concept: Load the data\n",
    "\n",
    "The purpose of this section is to load in the data that will be used in this example.\n",
    "\n",
    "The Numenta Anomaly Benchmark (NAB) dataset is used in this example. It's location is identified via a master url.\n",
    "\n",
    "Two csv files from this dataset are used. Both being identified by url extension suffix, which is concatenated with the master url when being read in. Pandas is used to read in both csv files, which are stored in Pandas Dataframes (https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html). \n",
    "\n",
    "art_daily_small_noise.csv will be used for training.\n",
    "\n",
    "art_daily_jumpsup.csv will be used for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ec21b1",
   "metadata": {},
   "source": [
    "<hr style=\\\"border-top: 1px solid rgb(0, 91, 94);\\\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7242887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_small_noise.head())\n",
    "\n",
    "print(df_daily_jumpsup.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a838d70c",
   "metadata": {},
   "source": [
    "#### Concept: Quick look at the data\n",
    "\n",
    "The purpose of this section is to simply print the data to check that it has loaded in correctly.\n",
    "\n",
    "When printing the data (which has been stored in Pandas Dataframes), the Pandas dataframe function \"pandas.DataFrame.head\" is called. This function will return the first n rows of the dataframe. As no n value is entered, the first five rows are returned, as five is the default value (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a5ffc8",
   "metadata": {},
   "source": [
    "<hr style=\\\"border-top: 1px solid rgb(0, 91, 94);\\\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba232ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df_small_noise.plot(legend=False, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e55eb51",
   "metadata": {},
   "source": [
    "#### Concept: Visualize timeseries data without anomalies\n",
    "\n",
    "The purpose of this section is to visualize the timeseries data without anomalies.\n",
    "\n",
    "To achieve this, a plot of the data from art_daily_small_noise.csv is created using Matplotlib. This provides a visualisation of the data that will be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928095fb",
   "metadata": {},
   "source": [
    "<hr style=\\\"border-top: 1px solid rgb(0, 91, 94);\\\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beebed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df_daily_jumpsup.plot(legend=False, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae16a1ef",
   "metadata": {},
   "source": [
    "#### Concept: Visualize timeseries data with anomalies\n",
    "\n",
    "The purpose of this section is to visualize the timeseries data with anomalies.\n",
    "\n",
    "To achieve this, a plot of the data from art_daily_jumpsup.csv is created using Matplotlib. This provides a visualisation of the data that will be used for testing. We will test if the sudden jump seen in the visualisation will be detected as an anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc34db",
   "metadata": {},
   "source": [
    "<hr style=\\\"border-top: 1px solid rgb(0, 91, 94);\\\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0448bb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and save the mean and std we get,\n",
    "# for normalizing test data.\n",
    "training_mean = df_small_noise.mean()\n",
    "training_std = df_small_noise.std()\n",
    "df_training_value = (df_small_noise - training_mean) / training_std\n",
    "print(\"Number of training samples:\", len(df_training_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4984899",
   "metadata": {},
   "source": [
    "#### Concept: Prepare training data\n",
    "\n",
    "The purpose of this section is to prepare the art_daily_small_noise.csv data for use in training by normalizing it.\n",
    "\n",
    "pandas.DataFrame.mean is used to identify to mean of the training data (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mean.html).\n",
    "\n",
    "pandas.DataFrame.std is used to identify the standard deviation of the training data (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.std.html).\n",
    "\n",
    "The training data is then normalized using the following calculation: (df_small_noise - training_mean) / training_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27540b90",
   "metadata": {},
   "source": [
    "<hr style=\\\"border-top: 1px solid rgb(0, 91, 94);\\\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8960b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEPS = 288\n",
    "\n",
    "# Generated training sequences for use in the model.\n",
    "def create_sequences(values, time_steps=TIME_STEPS):\n",
    "    output = []\n",
    "    for i in range(len(values) - time_steps + 1):\n",
    "        output.append(values[i : (i + time_steps)])\n",
    "    return np.stack(output)\n",
    "\n",
    "\n",
    "x_train = create_sequences(df_training_value.values)\n",
    "print(\"Training input shape: \", x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad8fbe0",
   "metadata": {},
   "source": [
    "#### Concept: Create sequences\n",
    "\n",
    "The purpose of this section is to create sequences combining TIME_STEPS contiguous data values from the training data.\n",
    "\n",
    "Here, a function \"create_sequences\" is defined, with the training data values and time_steps value as input parameters.\n",
    "\n",
    "In this example, there is a value for every five minutes for fourteen days, therefore time_steps = 24 * 60 / 5 = 288 timesteps per day\n",
    "\n",
    "Each item in the sequence is then calculated using a for loop and appended to the output array.\n",
    "\n",
    "numpy.stack is used to join the output array together as a sequence and is returned by the function (https://numpy.org/doc/stable/reference/generated/numpy.stack.html).\n",
    "\n",
    "After defining the function, it is called using the training data values as input, with the outputted sequence being assigned to the x_train variable.\n",
    "\n",
    "numpy.shape is then used to print the shape of the training data sequence (https://numpy.org/doc/stable/reference/generated/numpy.shape.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dca4ed7",
   "metadata": {},
   "source": [
    "<hr style=\\\"border-top: 1px solid rgb(0, 91, 94);\\\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdda23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(x_train.shape[1], x_train.shape[2])),\n",
    "        layers.Conv1D(\n",
    "            filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv1D(\n",
    "            filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Conv1DTranspose(\n",
    "            filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv1DTranspose(\n",
    "            filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876391c9",
   "metadata": {},
   "source": [
    "#### Concept: Build a model\n",
    "\n",
    "The purpose of this section is to build a convolutional reconstruction autoencoder model.The model will take input of shape (batch_size, sequence_length, num_features) and return output of the same shape. In this case, sequence_length is 288 and num_features is 1.\n",
    "\n",
    "keras.Sequential groups a linear stack of layers into a tf.keras.Model (https://www.tensorflow.org/api_docs/python/tf/keras/Sequential).\n",
    "\n",
    "layers.Input is used to instantiate a Keras tensor (an object from the underlying backend) (http://man.hubwiz.com/docset/TensorFlow.docset/Contents/Resources/Documents/api_docs/python/tf/keras/layers/Input.html).\n",
    "\n",
    "layers.Conv1D creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs (http://man.hubwiz.com/docset/TensorFlow.docset/Contents/Resources/Documents/api_docs/python/tf/keras/layers/Conv1D.html).\n",
    "\n",
    "layers.Dropout creates the dropout layer which randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout).\n",
    "\n",
    "layers.Conv1DTranspose creates a transposed convolution layer (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1DTranspose).\n",
    "\n",
    "model.compile configures the model for training (https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile).\n",
    "\n",
    "model.summary prints a string summary of the model network (https://www.tensorflow.org/api_docs/python/tf/keras/Model#summary)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff2be20",
   "metadata": {},
   "source": [
    "<hr style=\\\"border-top: 1px solid rgb(0, 91, 94);\\\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4593369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train,\n",
    "    x_train,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac292fd",
   "metadata": {},
   "source": [
    "#### Concept: Train the model\n",
    "\n",
    "The purpose of this section is to train the model.\n",
    "\n",
    "model.fit trains the model for a fixed number of epochs (in this case 50) (https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit).\n",
    "\n",
    "keras.callbacks.EarlyStopping stops training when a monitored metric has stopped improving (in this case \"val_loss\") (https://keras.io/api/callbacks/early_stopping/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dca3357",
   "metadata": {},
   "source": [
    "<hr style=\\\"border-top: 1px solid rgb(0, 91, 94);\\\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65b5a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f655ee",
   "metadata": {},
   "source": [
    "#### Concept: Plot training and validation loss\n",
    "\n",
    "The purpose of this section is to plot the training loss history and the validation loss history.\n",
    "\n",
    "history.history is a callback that records events into a History object (return from the model.fit method) (https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History).\n",
    "\n",
    "Matplotlib is used to plot these histories for \"Training Loss\" and \"Validation Loss\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61a9b92",
   "metadata": {},
   "source": [
    "<hr style=\\\"border-top: 1px solid rgb(0, 91, 94);\\\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba7b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train MAE loss.\n",
    "x_train_pred = model.predict(x_train)\n",
    "train_mae_loss = np.mean(np.abs(x_train_pred - x_train), axis=1)\n",
    "\n",
    "plt.hist(train_mae_loss, bins=50)\n",
    "plt.xlabel(\"Train MAE loss\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()\n",
    "\n",
    "# Get reconstruction loss threshold.\n",
    "threshold = np.max(train_mae_loss)\n",
    "print(\"Reconstruction error threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5e2358",
   "metadata": {},
   "source": [
    "#### Concept: Detecting anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53e1a4f",
   "metadata": {},
   "source": [
    "<hr style=\\\"border-top: 1px solid rgb(0, 91, 94);\\\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b573e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking how the first sequence is learnt\n",
    "plt.plot(x_train[0])\n",
    "plt.plot(x_train_pred[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9aa58",
   "metadata": {},
   "source": [
    "#### Concept: Compare recontruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe33ee30",
   "metadata": {},
   "source": [
    "<hr style=\\\"border-top: 1px solid rgb(0, 91, 94);\\\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a30485",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_value = (df_daily_jumpsup - training_mean) / training_std\n",
    "fig, ax = plt.subplots()\n",
    "df_test_value.plot(legend=False, ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# Create sequences from test values.\n",
    "x_test = create_sequences(df_test_value.values)\n",
    "print(\"Test input shape: \", x_test.shape)\n",
    "\n",
    "# Get test MAE loss.\n",
    "x_test_pred = model.predict(x_test)\n",
    "test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)\n",
    "test_mae_loss = test_mae_loss.reshape((-1))\n",
    "\n",
    "plt.hist(test_mae_loss, bins=50)\n",
    "plt.xlabel(\"test MAE loss\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()\n",
    "\n",
    "# Detect all the samples which are anomalies.\n",
    "anomalies = test_mae_loss > threshold\n",
    "print(\"Number of anomaly samples: \", np.sum(anomalies))\n",
    "print(\"Indices of anomaly samples: \", np.where(anomalies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103250a7",
   "metadata": {},
   "source": [
    "#### Concept: Prepare test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58296f2",
   "metadata": {},
   "source": [
    "<hr style=\\\"border-top: 1px solid rgb(0, 91, 94);\\\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974e9039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data i is an anomaly if samples [(i - timesteps + 1) to (i)] are anomalies\n",
    "anomalous_data_indices = []\n",
    "for data_idx in range(TIME_STEPS - 1, len(df_test_value) - TIME_STEPS + 1):\n",
    "    if np.all(anomalies[data_idx - TIME_STEPS + 1 : data_idx]):\n",
    "        anomalous_data_indices.append(data_idx)\n",
    "        \n",
    "df_subset = df_daily_jumpsup.iloc[anomalous_data_indices]\n",
    "fig, ax = plt.subplots()\n",
    "df_daily_jumpsup.plot(legend=False, ax=ax)\n",
    "df_subset.plot(legend=False, ax=ax, color=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8526abde",
   "metadata": {},
   "source": [
    "#### Concept: Plot anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3727d054",
   "metadata": {},
   "source": [
    "<hr style=\\\"border-top: 1px solid rgb(0, 91, 94);\\\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6799c735",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0aa123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
