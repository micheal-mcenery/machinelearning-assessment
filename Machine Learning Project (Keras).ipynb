{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6759213",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:3rem;color:rgb(0, 91, 94);text-align:center;\">Machine Learning Project (Keras)</h1>\n",
    "<hr style=\\\"border-top: 1px solid rgb(0, 91, 94);\\\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be96801",
   "metadata": {},
   "source": [
    "This section of the project has four key requirements, each of which have been satisfied below:\n",
    "\n",
    "- On the keras website, there is an example of time-series anomaly detection. Re-create this example in a notebook of your own, explaining the concepts.\n",
    "\n",
    "- Clearly explain each keras function used, referring to the documentation.\n",
    "\n",
    "- Include an introduction to your notebook, setting the context and describing what the reader can expect as they read down through the notebook.\n",
    "\n",
    "- Include a conclusion section where you suggest improvements you could make to the analysis in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11332a9f",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The purpose of this notebook is to recreate the time-series anomaly detection example found on the Keras website, explain the main concepts, and define the purpose of each of the functions used. \"Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow. It was developed with a focus on enabling fast experimentation\" (https://keras.io/about/). For each section of the code below, both the concept of the section is exaplained, as well as any keras functions found within."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edf7dac",
   "metadata": {},
   "source": [
    "### Recreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b15792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e15190f",
   "metadata": {},
   "source": [
    "#### Concept: Setup\n",
    "\n",
    "The purpose of this section is to import the different tools that will be needed in this example.\n",
    "\n",
    "Numpy, Pandas, Keras (Layers), and Matplotlib are imported for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e0113e",
   "metadata": {},
   "source": [
    "#### Keras Functions:\n",
    "\n",
    "- from tensorflow import keras (imports Keras functionality)\n",
    "- from tensorflow.keras import layers (imports Keras Layers, basic building blocks of neural networks, https://keras.io/api/layers/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e6d8f1",
   "metadata": {},
   "source": [
    "<hr style=\\\"border-top: 1px solid rgb(0, 91, 94);\\\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f37c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_url_root = \"https://raw.githubusercontent.com/numenta/NAB/master/data/\"\n",
    "\n",
    "df_small_noise_url_suffix = \"artificialNoAnomaly/art_daily_small_noise.csv\"\n",
    "df_small_noise_url = master_url_root + df_small_noise_url_suffix\n",
    "df_small_noise = pd.read_csv(\n",
    "    df_small_noise_url, parse_dates=True, index_col=\"timestamp\"\n",
    ")\n",
    "\n",
    "df_daily_jumpsup_url_suffix = \"artificialWithAnomaly/art_daily_jumpsup.csv\"\n",
    "df_daily_jumpsup_url = master_url_root + df_daily_jumpsup_url_suffix\n",
    "df_daily_jumpsup = pd.read_csv(\n",
    "    df_daily_jumpsup_url, parse_dates=True, index_col=\"timestamp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151b77c7",
   "metadata": {},
   "source": [
    "#### Concept: Load the data\n",
    "\n",
    "The purpose of this section is to load in the data that will be used in this example.\n",
    "\n",
    "The Numenta Anomaly Benchmark (NAB) dataset is used in this example. It's location is identified via a master url.\n",
    "\n",
    "Two csv files from this dataset are used. Both being identified by url extension suffix, which is concatenated with the master url when being read in. Pandas is used to read in both csv files, which are stored in Pandas Dataframes (https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html). \n",
    "\n",
    "art_daily_small_noise.csv will be used for training.\n",
    "\n",
    "art_daily_jumpsup.csv will be used for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ec21b1",
   "metadata": {},
   "source": [
    "<hr style=\\\"border-top: 1px solid rgb(0, 91, 94);\\\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7242887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_small_noise.head())\n",
    "\n",
    "print(df_daily_jumpsup.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a838d70c",
   "metadata": {},
   "source": [
    "#### Concept: Quick look at the data\n",
    "\n",
    "The purpose of this section is to simply print the data to check that it has loaded in correctly.\n",
    "\n",
    "When printing the data (which has been stored in Pandas Dataframes), the Pandas dataframe function \"pandas.DataFrame.head\" is called. This function will return the first n rows of the dataframe. As no n value is entered, the first five rows are returned, as five is the default value (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a5ffc8",
   "metadata": {},
   "source": [
    "<hr style=\\\"border-top: 1px solid rgb(0, 91, 94);\\\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba232ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df_small_noise.plot(legend=False, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e55eb51",
   "metadata": {},
   "source": [
    "#### Concept: Visualize timeseries data without anomalies\n",
    "\n",
    "The purpose of this section is to visualize the timeseries data without anomalies.\n",
    "\n",
    "To achieve this, a plot of the data from art_daily_small_noise.csv is created using Matplotlib. This provides a visualisation of the data that will be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928095fb",
   "metadata": {},
   "source": [
    "<hr style=\\\"border-top: 1px solid rgb(0, 91, 94);\\\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beebed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df_daily_jumpsup.plot(legend=False, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae16a1ef",
   "metadata": {},
   "source": [
    "#### Concept: Visualize timeseries data with anomalies\n",
    "\n",
    "The purpose of this section is to visualize the timeseries data with anomalies.\n",
    "\n",
    "To achieve this, a plot of the data from art_daily_jumpsup.csv is created using Matplotlib. This provides a visualisation of the data that will be used for testing. We will test if the sudden jump seen in the visualisation will be detected as an anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc34db",
   "metadata": {},
   "source": [
    "<hr style=\\\"border-top: 1px solid rgb(0, 91, 94);\\\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd5cdb5",
   "metadata": {},
   "source": [
    "#### Concept: Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0448bb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and save the mean and std we get,\n",
    "# for normalizing test data.\n",
    "training_mean = df_small_noise.mean()\n",
    "training_std = df_small_noise.std()\n",
    "df_training_value = (df_small_noise - training_mean) / training_std\n",
    "print(\"Number of training samples:\", len(df_training_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924cfe53",
   "metadata": {},
   "source": [
    "#### Concept: Create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8960b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEPS = 288\n",
    "\n",
    "# Generated training sequences for use in the model.\n",
    "def create_sequences(values, time_steps=TIME_STEPS):\n",
    "    output = []\n",
    "    for i in range(len(values) - time_steps + 1):\n",
    "        output.append(values[i : (i + time_steps)])\n",
    "    return np.stack(output)\n",
    "\n",
    "\n",
    "x_train = create_sequences(df_training_value.values)\n",
    "print(\"Training input shape: \", x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de542c42",
   "metadata": {},
   "source": [
    "#### Concept: Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdda23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(x_train.shape[1], x_train.shape[2])),\n",
    "        layers.Conv1D(\n",
    "            filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv1D(\n",
    "            filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Conv1DTranspose(\n",
    "            filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv1DTranspose(\n",
    "            filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "        ),\n",
    "        layers.Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a40be",
   "metadata": {},
   "source": [
    "#### Concept: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4593369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train,\n",
    "    x_train,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd21c85",
   "metadata": {},
   "source": [
    "#### Concept: Plot training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65b5a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4287c7",
   "metadata": {},
   "source": [
    "#### Concept: Detecting anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba7b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train MAE loss.\n",
    "x_train_pred = model.predict(x_train)\n",
    "train_mae_loss = np.mean(np.abs(x_train_pred - x_train), axis=1)\n",
    "\n",
    "plt.hist(train_mae_loss, bins=50)\n",
    "plt.xlabel(\"Train MAE loss\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()\n",
    "\n",
    "# Get reconstruction loss threshold.\n",
    "threshold = np.max(train_mae_loss)\n",
    "print(\"Reconstruction error threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8fa252",
   "metadata": {},
   "source": [
    "#### Concept: Compare recontruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b573e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking how the first sequence is learnt\n",
    "plt.plot(x_train[0])\n",
    "plt.plot(x_train_pred[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec44011",
   "metadata": {},
   "source": [
    "#### Concept: Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a30485",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_value = (df_daily_jumpsup - training_mean) / training_std\n",
    "fig, ax = plt.subplots()\n",
    "df_test_value.plot(legend=False, ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# Create sequences from test values.\n",
    "x_test = create_sequences(df_test_value.values)\n",
    "print(\"Test input shape: \", x_test.shape)\n",
    "\n",
    "# Get test MAE loss.\n",
    "x_test_pred = model.predict(x_test)\n",
    "test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)\n",
    "test_mae_loss = test_mae_loss.reshape((-1))\n",
    "\n",
    "plt.hist(test_mae_loss, bins=50)\n",
    "plt.xlabel(\"test MAE loss\")\n",
    "plt.ylabel(\"No of samples\")\n",
    "plt.show()\n",
    "\n",
    "# Detect all the samples which are anomalies.\n",
    "anomalies = test_mae_loss > threshold\n",
    "print(\"Number of anomaly samples: \", np.sum(anomalies))\n",
    "print(\"Indices of anomaly samples: \", np.where(anomalies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0727e2ab",
   "metadata": {},
   "source": [
    "#### Concept: Plot anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974e9039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data i is an anomaly if samples [(i - timesteps + 1) to (i)] are anomalies\n",
    "anomalous_data_indices = []\n",
    "for data_idx in range(TIME_STEPS - 1, len(df_test_value) - TIME_STEPS + 1):\n",
    "    if np.all(anomalies[data_idx - TIME_STEPS + 1 : data_idx]):\n",
    "        anomalous_data_indices.append(data_idx)\n",
    "        \n",
    "df_subset = df_daily_jumpsup.iloc[anomalous_data_indices]\n",
    "fig, ax = plt.subplots()\n",
    "df_daily_jumpsup.plot(legend=False, ax=ax)\n",
    "df_subset.plot(legend=False, ax=ax, color=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c52085c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6799c735",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0aa123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
